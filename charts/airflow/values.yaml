airflowChart:
  # Kubernetes API (for KubernetesExecutor)
  kubeAPI:
    serviceIP: "10.233.0.1"
    port: 443
  airflow:
    image:
      # OpenMetadata's Airflow image: includes managed-apis plugin + ingestion
      # pre-installed with compatible dependencies (pendulum, etc.)
      repository: docker.getcollate.io/openmetadata/ingestion
      tag: 1.5.4
      pullPolicy: IfNotPresent

    executor: KubernetesExecutor
    
    # Extra packages (OpenMetadata plugin is pre-installed in the image)
    extraPipPackages:
      - "boto3>=1.28.0"
      - "requests>=2.31.0"
      - "kubernetes>=28.1.0"
      # Needed for S3/MinIO remote logging handler
      - "apache-airflow-providers-amazon>=8.0.0,<9.0.0"
      # Fix collate-sqllineage requiring sqlparse<=0.5 (image has 0.5.5)
      - "sqlparse==0.5.0"

    # Init container (runs AFTER dags-git-clone) to:
    # 1. Make the git-synced repo/ dir writable for OpenMetadata DAG .py deploy
    # 2. Create dag_generated_configs/ for OpenMetadata JSON config files
    extraInitContainers:
      - name: fix-dags-permissions
        image: busybox:1.36
        command:
          - sh
          - -c
          - |
            chmod -R 777 /opt/airflow/dags/repo/ 2>/dev/null || true
            mkdir -p /opt/airflow/dags/dag_generated_configs
            chmod 777 /opt/airflow/dags/dag_generated_configs
        volumeMounts:
          - name: dags-data
            mountPath: /opt/airflow/dags

    # KubernetesExecutor pod template with install-pip-packages + fix-dags-permissions
    kubernetesPodTemplate:
      stringOverride: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: dummy-name
        spec:
          restartPolicy: Never
          serviceAccountName: airflow
          shareProcessNamespace: false
          securityContext:
            fsGroup: 0
          initContainers:
            - name: dags-git-clone
              image: registry.k8s.io/git-sync/git-sync:v3.6.9
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 65533
                runAsGroup: 65533
              envFrom:
                - secretRef:
                    name: airflow-config-envs
              env:
                - name: GIT_SYNC_ONE_TIME
                  value: "true"
                - name: GIT_SYNC_ROOT
                  value: "/dags"
                - name: GIT_SYNC_DEST
                  value: "repo"
                - name: GIT_SYNC_REPO
                  value: "https://github.com/ibrahimlahouar/airflow-dags.git"
                - name: GIT_SYNC_BRANCH
                  value: "main"
                - name: GIT_SYNC_REV
                  value: "HEAD"
                - name: GIT_SYNC_DEPTH
                  value: "1"
                - name: GIT_SYNC_WAIT
                  value: "60"
                - name: GIT_SYNC_TIMEOUT
                  value: "120"
                - name: GIT_SYNC_ADD_USER
                  value: "true"
                - name: GIT_SYNC_MAX_SYNC_FAILURES
                  value: "0"
                - name: GIT_SYNC_SUBMODULES
                  value: "recursive"
                - name: GIT_KNOWN_HOSTS
                  value: "false"
                - name: DATABASE_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-shared-secrets
                      key: airflow-password
                - name: CONNECTION_CHECK_MAX_COUNT
                  value: "0"
              volumeMounts:
                - name: dags-data
                  mountPath: /dags
            - name: install-pip-packages
              image: docker.getcollate.io/openmetadata/ingestion:1.5.4
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 50000
                runAsGroup: 0
              command: ["/bin/bash", "-c", "pip install --target=/opt/airflow/dags/.local/lib/python3.10/site-packages 'apache-airflow-providers-amazon>=8.0.0,<9.0.0' 'apache-airflow-providers-common-sql<1.10.0' 'sqlparse==0.5.0' && echo Done"]
              envFrom:
                - secretRef:
                    name: airflow-config-envs
              env:
                - name: PYTHONPATH
                  value: "/opt/airflow/dags/.local/lib/python3.10/site-packages"
                - name: DATABASE_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-shared-secrets
                      key: airflow-password
                - name: CONNECTION_CHECK_MAX_COUNT
                  value: "0"
              volumeMounts:
                - name: dags-data
                  mountPath: /opt/airflow/dags
            - name: fix-dags-permissions
              image: busybox:1.36
              command: ["sh", "-c", "chmod -R 777 /opt/airflow/dags/repo/ 2>/dev/null || true; mkdir -p /opt/airflow/dags/dag_generated_configs; chmod 777 /opt/airflow/dags/dag_generated_configs"]
              volumeMounts:
                - name: dags-data
                  mountPath: /opt/airflow/dags
          containers:
            - name: base
              image: docker.getcollate.io/openmetadata/ingestion:1.5.4
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 50000
                runAsGroup: 0
              envFrom:
                - secretRef:
                    name: airflow-config-envs
              env:
                - name: PYTHONPATH
                  value: "/opt/airflow/dags/.local/lib/python3.10/site-packages"
                - name: AIRFLOW__CORE__EXECUTOR
                  value: LocalExecutor
                - name: DATABASE_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: postgres-shared-secrets
                      key: airflow-password
                - name: CONNECTION_CHECK_MAX_COUNT
                  value: "20"
              volumeMounts:
                - name: dags-data
                  mountPath: /opt/airflow/dags
                - name: logs-data
                  mountPath: /opt/airflow/logs
          volumes:
            - name: dags-data
              emptyDir: {}
            - name: logs-data
              emptyDir: {}

    config:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      # OpenMetadata managed APIs - writable directory for generated DAG configs (JSON)
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dags/dag_generated_configs"
      # Remote logging to MinIO (S3-compatible)
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://warehouse/airflow-logs"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "minio_logs"
      AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"

    # Airflow connections (created automatically by the Helm chart)
    connections:
      - id: minio_logs
        type: aws
        description: "MinIO S3 connection for Airflow remote logs"
        extra: |-
          {
            "aws_access_key_id": "minioadmin",
            "aws_secret_access_key": "minioadmin123",
            "region_name": "us-east-1",
            "endpoint_url": "http://minio.minio.svc:9000"
          }

    users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: Admin
      lastName: User

  web:
    service:
      type: NodePort
      nodePort:
        http: 32180

  postgresql:
    enabled: false

  externalDatabase:
    type: postgres
    host: postgres-shared-postgresql.infra.svc.cluster.local
    port: 5432
    database: airflow_db
    user: airflow_user
    passwordSecret: postgres-shared-secrets
    passwordSecretKey: airflow-password

  serviceAccount:
    create: true
    name: "airflow"

  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: "https://github.com/ibrahimlahouar/airflow-dags.git"
      branch: "main"
      subPath: ""
      # public repo -> no credentials required
      sshSecret: ""
      httpSecret: ""

  ingress:
    enabled: true
    web:
      path: ""
      host: "airflow.data-platform.local"
      annotations:
        kubernetes.io/ingress.class: nginx
  
  logs:
    persistence:
      enabled: false

  scheduler:
    logCleanup:
      enabled: false

  workers:
    enabled: false
    logCleanup:
      enabled: false

  # Enable migration job explicitly
  migrateDatabaseJob:
    enabled: true
    jobAnnotations:
      "argocd.argoproj.io/hook": "PreSync"
      "argocd.argoproj.io/hook-delete-policy": "BeforeHookCreation"

  redis:
    enabled: false

  flower:
    enabled: false

  pgbouncer:
    enabled: false
