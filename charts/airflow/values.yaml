airflowChart:
  # Kubernetes API (for KubernetesExecutor)
  kubeAPI:
    serviceIP: "10.233.0.1"
    port: 443
  airflow:
    image:
      # OpenMetadata's Airflow image: includes managed-apis plugin + ingestion
      # pre-installed with compatible dependencies (pendulum, etc.)
      repository: docker.getcollate.io/openmetadata/ingestion
      tag: 1.5.4
      pullPolicy: IfNotPresent

    executor: KubernetesExecutor
    
    # Extra packages (OpenMetadata plugin is pre-installed in the image)
    extraPipPackages:
      - "boto3>=1.28.0"
      - "requests>=2.31.0"
      - "kubernetes>=28.1.0"
      # Needed for S3/MinIO remote logging handler
      - "apache-airflow-providers-amazon>=8.0.0,<9.0.0"
      # Fix collate-sqllineage requiring sqlparse<=0.5 (image has 0.5.5)
      - "sqlparse==0.5.0"
        
    config:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      # OpenMetadata managed APIs - writable directory for generated DAG configs (JSON)
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dags/dag_generated_configs"
      # Remote logging to MinIO (S3-compatible)
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://warehouse/airflow-logs"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "minio_logs"
      AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"

    # Airflow connections (created automatically by the Helm chart)
    connections:
      - id: minio_logs
        type: aws
        description: "MinIO S3 connection for Airflow remote logs"
        extra: |-
          {
            "aws_access_key_id": "minioadmin",
            "aws_secret_access_key": "minioadmin123",
            "region_name": "us-east-1",
            "endpoint_url": "http://minio.minio.svc:9000"
          }

    users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: Admin
      lastName: User

  web:
    service:
      type: NodePort
      nodePort:
        http: 32180

  postgresql:
    enabled: false

  externalDatabase:
    type: postgres
    host: postgres-shared-postgresql.infra.svc.cluster.local
    port: 5432
    database: airflow_db
    user: airflow_user
    passwordSecret: postgres-shared-secrets
    passwordSecretKey: airflow-password

  serviceAccount:
    create: true
    name: "airflow"

  dags:
    # Use the emptyDir root (writable) instead of default repo/ (read-only git-sync)
    # Airflow scans recursively: finds DAGs in repo/ and dag_generated_configs/
    path: /opt/airflow/dags
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: "https://github.com/ibrahimlahouar/airflow-dags.git"
      branch: "main"
      subPath: ""
      # public repo -> no credentials required
      sshSecret: ""
      httpSecret: ""

  ingress:
    enabled: true
    web:
      path: ""
      host: "airflow.data-platform.local"
      annotations:
        kubernetes.io/ingress.class: nginx
  
  logs:
    persistence:
      enabled: false

  # Init container to create the writable directory for OpenMetadata-generated DAGs
  # (the dags volume is an emptyDir populated by git-sync, so this dir must be recreated on each pod start)
  extraInitContainers:
    - name: create-dag-configs-dir
      image: busybox:1.36
      command: ["sh", "-c", "mkdir -p /opt/airflow/dags/dag_generated_configs && chmod 777 /opt/airflow/dags/dag_generated_configs"]
      volumeMounts:
        - name: dags-data
          mountPath: /opt/airflow/dags

  scheduler:
    logCleanup:
      enabled: false

  workers:
    enabled: false
    logCleanup:
      enabled: false

  # Enable migration job explicitly
  migrateDatabaseJob:
    enabled: true
    jobAnnotations:
      "argocd.argoproj.io/hook": "PreSync"
      "argocd.argoproj.io/hook-delete-policy": "BeforeHookCreation"

  redis:
    enabled: false

  flower:
    enabled: false

  pgbouncer:
    enabled: false
