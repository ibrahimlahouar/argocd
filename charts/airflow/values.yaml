airflowChart:
  # Kubernetes API (for KubernetesExecutor)
  kubeAPI:
    serviceIP: "10.233.0.1"
    port: 443
  airflow:
    image:
      repository: apache/airflow
      tag: 2.7.3-python3.10
      pullPolicy: IfNotPresent

    executor: KubernetesExecutor
    
    # Packages needed for the crypto DAG (S3/MinIO, HTTP requests, Kubernetes API)
    extraPipPackages:
      - "boto3>=1.28.0"
      - "requests>=2.31.0"
      - "kubernetes>=28.1.0"
      # Needed for S3/MinIO remote logging handler
      - "apache-airflow-providers-amazon>=8.0.0,<9.0.0"
      # OpenMetadata Managed APIs plugin for Airflow integration
      # v1.3.0.0: pendulum~=2.1.2 (compatible with Airflow 2.7.3) + fixed Airflow 2.7 imports
      # v1.5.4+ requires pendulum~=3.0 (incompatible with Airflow 2.7.3)
      - "openmetadata-managed-apis==1.3.0.0"
        
    config:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      # Remote logging to MinIO (S3-compatible)
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://warehouse/airflow-logs"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "minio_logs"
      AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"

    # Airflow connections (created automatically by the Helm chart)
    connections:
      - id: minio_logs
        type: aws
        description: "MinIO S3 connection for Airflow remote logs"
        extra: |-
          {
            "aws_access_key_id": "minioadmin",
            "aws_secret_access_key": "minioadmin123",
            "region_name": "us-east-1",
            "endpoint_url": "http://minio.minio.svc:9000"
          }

    users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: Admin
      lastName: User

  web:
    service:
      type: NodePort
      nodePort:
        http: 32180

  postgresql:
    enabled: false

  externalDatabase:
    type: postgres
    host: postgres-shared-postgresql.infra.svc.cluster.local
    port: 5432
    database: airflow_db
    user: airflow_user
    passwordSecret: postgres-shared-secrets
    passwordSecretKey: airflow-password

  serviceAccount:
    create: true
    name: "airflow"

  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: "https://github.com/ibrahimlahouar/airflow-dags.git"
      branch: "main"
      subPath: ""
      # public repo -> no credentials required
      sshSecret: ""
      httpSecret: ""

  ingress:
    enabled: true
    web:
      path: ""
      host: "airflow.data-platform.local"
      annotations:
        kubernetes.io/ingress.class: nginx
  
  logs:
    persistence:
      enabled: false

  scheduler:
    logCleanup:
      enabled: false

  workers:
    enabled: false
    logCleanup:
      enabled: false

  # Enable migration job explicitly
  migrateDatabaseJob:
    enabled: true
    jobAnnotations:
      "argocd.argoproj.io/hook": "PreSync"
      "argocd.argoproj.io/hook-delete-policy": "BeforeHookCreation"

  redis:
    enabled: false

  flower:
    enabled: false

  pgbouncer:
    enabled: false
